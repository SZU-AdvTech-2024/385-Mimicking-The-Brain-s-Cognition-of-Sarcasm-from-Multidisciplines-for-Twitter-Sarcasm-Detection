-------------------------------hyperparameters---------------------------
lr: 0.0001, batch-size: 64, 
-------------------------------------------------------------------------
**train** epoch 0, train_acc: 0.7056419055308841, train_loss: 139.71786499023438, test_acc: 0.7185554171855542, test_P: 0.705710102489019, test_R: 0.502606882168926, test_F1: 0.587088915956151
**train** epoch 1, train_acc: 0.7571154622527251, train_loss: 123.26985931396484, test_acc: 0.7388958073889581, test_P: 0.6656626506024096, test_R: 0.6913451511991658, test_F1: 0.6782608695652174
**train** epoch 2, train_acc: 0.7618591037545418, train_loss: 118.53223419189453, test_acc: 0.7388958073889581, test_P: 0.6636904761904762, test_R: 0.6976016684045881, test_F1: 0.6802236908998475
**train** epoch 3, train_acc: 0.7687222446507872, train_loss: 115.85088348388672, test_acc: 0.7368202573682026, test_P: 0.6712328767123288, test_R: 0.6642335766423357, test_F1: 0.6677148846960168
**train** epoch 4, train_acc: 0.7676624949535729, train_loss: 115.03280639648438, test_acc: 0.7293482772934827, test_P: 0.6856106408706167, test_R: 0.5912408759124088, test_F1: 0.6349384098544233
**train** epoch 5, train_acc: 0.7700343157044812, train_loss: 113.68695831298828, test_acc: 0.7368202573682026, test_P: 0.6591576885406464, test_R: 0.7017726798748697, test_F1: 0.6797979797979798
**train** epoch 6, train_acc: 0.7730621719822366, train_loss: 112.73822784423828, test_acc: 0.7397260273972602, test_P: 0.6762208067940552, test_R: 0.6642335766423357, test_F1: 0.6701735928458706
**train** epoch 7, train_acc: 0.7730621719822366, train_loss: 111.66924285888672, test_acc: 0.7418015774180158, test_P: 0.6487202118270079, test_R: 0.7664233576642335, test_F1: 0.7026768642447419
**train** epoch 8, train_acc: 0.7744751715785224, train_loss: 110.24419403076172, test_acc: 0.7372353673723536, test_P: 0.663, test_R: 0.6913451511991658, test_F1: 0.6768759571209801
**train** epoch 9, train_acc: 0.7777553492127574, train_loss: 109.508056640625, test_acc: 0.7364051473640515, test_P: 0.6443850267379679, test_R: 0.7539103232533889, test_F1: 0.6948582412301778
**train** epoch 10, train_acc: 0.7777048849414614, train_loss: 109.96892547607422, test_acc: 0.7297633872976339, test_P: 0.7031662269129287, test_R: 0.5557872784150156, test_F1: 0.6208503203261503
**train** epoch 11, train_acc: 0.7813887767460638, train_loss: 108.46965026855469, test_acc: 0.7422166874221668, test_P: 0.6585365853658537, test_R: 0.7320125130344108, test_F1: 0.6933333333333334
**train** epoch 12, train_acc: 0.7837101332256762, train_loss: 107.36995697021484, test_acc: 0.742631797426318, test_P: 0.6906636670416197, test_R: 0.6402502606882169, test_F1: 0.6645021645021645
**train** epoch 13, train_acc: 0.781943883730319, train_loss: 106.75666046142578, test_acc: 0.7434620174346201, test_P: 0.675954592363261, test_R: 0.6830031282586028, test_F1: 0.6794605809128631
**train** epoch 14, train_acc: 0.7822971336293904, train_loss: 106.7886734008789, test_acc: 0.7438771274387713, test_P: 0.7100737100737101, test_R: 0.602711157455683, test_F1: 0.6520022560631697
**train** epoch 15, train_acc: 0.7863342753330642, train_loss: 105.87572479248047, test_acc: 0.7409713574097135, test_P: 0.6468010517090271, test_R: 0.7695516162669447, test_F1: 0.7028571428571428
**train** epoch 16, train_acc: 0.7890593459830441, train_loss: 105.26579284667969, test_acc: 0.7355749273557493, test_P: 0.642226148409894, test_R: 0.7580813347236705, test_F1: 0.6953610712577714
**train** epoch 17, train_acc: 0.7934497375857893, train_loss: 103.44720458984375, test_acc: 0.7384806973848069, test_P: 0.6737064413938754, test_R: 0.6652763295099061, test_F1: 0.6694648478488983
**train** epoch 18, train_acc: 0.7914816310052483, train_loss: 103.67333221435547, test_acc: 0.734329597343296, test_P: 0.6587064676616915, test_R: 0.6903023983315955, test_F1: 0.6741344195519349
**train** epoch 19, train_acc: 0.796225272507065, train_loss: 102.14762115478516, test_acc: 0.7247820672478207, test_P: 0.6215106732348111, test_R: 0.7893639207507821, test_F1: 0.6954524575103354
**train** epoch 20, train_acc: 0.7951655228098506, train_loss: 102.07280731201172, test_acc: 0.7364051473640515, test_P: 0.6730769230769231, test_R: 0.656934306569343, test_F1: 0.6649076517150396
