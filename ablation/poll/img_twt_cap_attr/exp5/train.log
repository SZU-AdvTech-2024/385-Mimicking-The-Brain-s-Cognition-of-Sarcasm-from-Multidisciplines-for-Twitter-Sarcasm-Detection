-------------------------------hyperparameters---------------------------
lr: 0.0001, batch-size: 64, 
-------------------------------------------------------------------------
**train** epoch 0, train_acc: 0.906893419459023, train_loss: 66.72232055664062, test_acc: 0.9194686591946866, test_P: 0.8875379939209727, test_R: 0.913451511991658, test_F1: 0.9003083247687564
**train** epoch 1, train_acc: 0.9573072264836496, train_loss: 39.805973052978516, test_acc: 0.9115815691158157, test_P: 0.8993576017130621, test_R: 0.8759124087591241, test_F1: 0.8874801901743264
**train** epoch 2, train_acc: 0.963514331853048, train_loss: 35.17044448852539, test_acc: 0.9119966791199668, test_P: 0.8986125933831377, test_R: 0.8779979144942649, test_F1: 0.8881856540084389
**train** epoch 3, train_acc: 0.9617480823576907, train_loss: 34.72718811035156, test_acc: 0.9103362391033624, test_P: 0.8998923573735199, test_R: 0.8717413972888426, test_F1: 0.885593220338983
**train** epoch 4, train_acc: 0.9660880096891401, train_loss: 32.08494567871094, test_acc: 0.9165628891656289, test_P: 0.8672480620155039, test_R: 0.9332638164754953, test_F1: 0.8990457056755399
**train** epoch 5, train_acc: 0.9661384739604361, train_loss: 32.3238410949707, test_acc: 0.9157326691573267, test_P: 0.867704280155642, test_R: 0.9301355578727841, test_F1: 0.8978359335681932
**train** epoch 6, train_acc: 0.9633629390391603, train_loss: 32.55341339111328, test_acc: 0.9207139892071399, test_P: 0.8855421686746988, test_R: 0.9197080291970803, test_F1: 0.90230179028133
**train** epoch 7, train_acc: 0.9664412595882116, train_loss: 30.3657169342041, test_acc: 0.9182233291822333, test_P: 0.8856275303643725, test_R: 0.9124087591240876, test_F1: 0.8988186954288649
**train** epoch 8, train_acc: 0.9684598304400485, train_loss: 28.276287078857422, test_acc: 0.9190535491905355, test_P: 0.8858585858585859, test_R: 0.9144942648592284, test_F1: 0.8999486916367367
